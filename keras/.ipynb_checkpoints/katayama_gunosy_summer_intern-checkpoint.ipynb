{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampleをベースに特徴量を加えたりして，精度向上を目指す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import scipy as sc\n",
    "seed = random.seed(12345)\n",
    "\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')\n",
    "df_test = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 記事IDの他に，記事の長さ，スクロールの長さ，記事閲覧時間を使用する\n",
    "# そのままでは扱いづらいので０〜１００ -> 0 100~200 -> 1 のように新たにカテゴライズする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = []\n",
    "for v in df['scroll_length'].values:\n",
    "    value = list(map(float,v.replace('[','').replace(']','').split(',')))\n",
    "    value = [str(int(v//25)) for v in value]\n",
    "    V1.append('['+', '.join(value)+']')\n",
    "V=[]\n",
    "for v in df['session_length'].values:\n",
    "    value = list(map(float,v.replace('[','').replace(']','').split(',')))\n",
    "    value = [str(int(v//1)) for v in value]\n",
    "    V.append('['+', '.join(value)+']')\n",
    "    \n",
    "V2=[]\n",
    "for v in df['article_length'].values:\n",
    "    value = list(map(float,v.replace('[','').replace(']','').split(',')))\n",
    "    value = [str(int(v//50)) for v in value]\n",
    "    V2.append('['+', '.join(value)+']')\n",
    "    \n",
    "df['session_length_alt'] = V\n",
    "df['scroll_length_alt'] = V1\n",
    "df['article_length_alt'] = V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test data もdouyouni\n",
    "V = []\n",
    "for v in df_test['session_length'].values:\n",
    "    value = list(map(float,v.replace('[','').replace(']','').split(',')))\n",
    "    \n",
    "    value = [str(200//10) if i > 200 else \"0\" if i < 0 else str(i//10) for i in value]\n",
    "    value = ', '.join(value)\n",
    "    V.append('['+value+']')\n",
    "\n",
    "V1 = []\n",
    "for v in df_test['scroll_length'].values:\n",
    "    value = list(map(float,v.replace('[','').replace(']','').split(',')))\n",
    "    value = [str(5000//250) if i > 5000 else \"0\" if i < 0 else str(i//250) for i in value]\n",
    "    value = ', '.join(value)\n",
    "    V1.append('['+value+']')\n",
    "\n",
    "    \n",
    "V2 = []\n",
    "for v in df_test['article_length'].values:\n",
    "    value = list(map(float,v.replace('[','').replace(']','').split(',')))\n",
    "    value = [str(10000//500) if i > 10000 else \"0\" if i < 0 else str(i//500) for i in value]\n",
    "    value = ', '.join(value)\n",
    "    V2.append('['+value+']')\n",
    "    \n",
    "df_test['session_length_alt'] = V\n",
    "df_test['scroll_length_alt'] = V1\n",
    "df_test['article_length_alt'] = V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再利用するためにメソッドとして定義\n",
    "def preprocess_x(df, vectorizer, with_fit=False,name='article_ids'):\n",
    "\n",
    "    transformed_article_ids = []\n",
    "    for i, row in tqdm(df.iterrows(), desc='transform' + name, total=len(df)):\n",
    "        transformed_article_ids.append(json.loads(row[name]))\n",
    "\n",
    "    if with_fit:\n",
    "        vectorizer.fit(transformed_article_ids)\n",
    "\n",
    "    return vectorizer.transform(transformed_article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / valid 分割\n",
    "feature_num = 20\n",
    "train, valid = train_test_split(df, test_size=0.2, shuffle=True, random_state=seed)\n",
    "\n",
    "#記事ID\n",
    "train_x = preprocess_x(train, vectorizer, with_fit=True)\n",
    "train_y = np.asarray(train['age_range'])\n",
    "\n",
    "valid_x = preprocess_x(valid, vectorizer, with_fit=False)\n",
    "valid_y = np.asarray(valid['age_range'])\n",
    "test_x = preprocess_x(df_test, vectorizer, with_fit=False)\n",
    "\n",
    "#閲覧時間\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x, max_features=feature_num)\n",
    "train_x1 = preprocess_x(train, vectorizer, with_fit=True, name=\"session_length_alt\")\n",
    "train_y = np.asarray(train['age_range'])\n",
    "valid_x1 = preprocess_x(valid, vectorizer, with_fit=False,name='session_length_alt')\n",
    "valid_y = np.asarray(valid['age_range'])\n",
    "test_x1 = preprocess_x(df_test, vectorizer, with_fit=False,name='session_length_alt')\n",
    "\n",
    "#スクロールした長さ\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x, max_features=feature_num)\n",
    "train_x2 = preprocess_x(train, vectorizer, with_fit=True, name=\"scroll_length_alt\")\n",
    "valid_x2 = preprocess_x(valid, vectorizer, with_fit=False,name='scroll_length_alt')\n",
    "test_x2 = preprocess_x(df_test, vectorizer, with_fit=False,name='scroll_length_alt')\n",
    "\n",
    "#記事の長さ\n",
    "vectorizer = CountVectorizer(analyzer=lambda x: x, max_features=feature_num)\n",
    "train_x3 = preprocess_x(train, vectorizer, with_fit=True, name=\"article_length_alt\")\n",
    "valid_x3 = preprocess_x(valid, vectorizer, with_fit=False,name='article_length_alt')\n",
    "test_x3 = preprocess_x(df_test, vectorizer, with_fit=False,name='article_length_alt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方針1 age gender でマルチタスク\n",
    "# 方針2 ageでシングルタスク\n",
    "# 入力データもx,x1,x2,x3を色々組み合わせて試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender ラベルを作成\n",
    "train_y1 = np.asarray(train['gender_range'])\n",
    "valid_y1 = np.asarray(valid['gender_range'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multi task model\n",
    "\n",
    "def m(units=512):\n",
    "    i = Input(shape=(5000,))\n",
    "    x = Dense(units,activation='relu')(i)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(units//2,activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(units//4,activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #\n",
    "    i1 = Input(shape=(20,))\n",
    "    i2 = Input(shape=(20,))\n",
    "    i3 = Input(shape=(20,))\n",
    "    #\n",
    "    j1 = Dense(16,activation='relu')(i1)\n",
    "    j2 = Dense(16,activation='relu')(i2)\n",
    "    j3 = Dense(16,activation='relu')(i3)\n",
    "    #x = concatenate([x,j1])\n",
    "    x = concatenate([x,j1,j2,j3])\n",
    "    x1 = Dense(units//4,activation='relu')(x)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    x2 = Dense(1,activation='sigmoid',name='gender')(x1)\n",
    "    x1 = Dense(6,activation='softmax',name='age')(x1)\n",
    "    model = keras.Model(\n",
    "                                    #i,\n",
    "                                    #[i,i1],\n",
    "                                    [i,i1,i2,i3],\n",
    "                                    [x1,x2])\n",
    "    model.compile(\n",
    "                loss = {'age':'sparse_categorical_crossentropy','gender':'binary_crossentropy'},\n",
    "                optimizer=keras.optimizers.Adam(1e-04),\n",
    "                #optimizer = keras.optimizers.SGD(),\n",
    "                metrics=['accuracy'],\n",
    "                loss_weights = {'age':1,\n",
    "                                        'gender':0.25}\n",
    "                )\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "####\n",
    "#学習\n",
    "####\n",
    "\n",
    "for i in [32,64,128]:\n",
    "    print('batch_size',i)\n",
    "    model = m(units=1024)\n",
    "    hist = model.fit(\n",
    "        #train_x.toarray(),\n",
    "        #[train_x.toarray(),train_x1.toarray()],\n",
    "        #[train_x.toarray(),train_x2.toarray()],\n",
    "        #[train_x.toarray(),train_x3.toarray()],\n",
    "        [train_x.toarray(),train_x1.toarray(),train_x2.toarray(),train_x3.toarray()],\n",
    "        [train_y,train_y1],\n",
    "                 epochs=50,batch_size=i,verbose=0,\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_age_loss',verbose=2,patience=10)],\n",
    "                validation_data = (\n",
    "                    #valid_x.toarray(), \n",
    "                    #[valid_x.toarray(),valid_x1.toarray()],\n",
    "                    #[valid_x.toarray(),valid_x2.toarray()],\n",
    "                    #[valid_x.toarray(),valid_x3.toarray()],\n",
    "                    [valid_x.toarray(),valid_x1.toarray(),valid_x2.toarray(),valid_x3.toarray()],\n",
    "                    [valid_y,valid_y1]))\n",
    "    \n",
    "    print(model.evaluate([valid_x.toarray(),valid_x1.toarray(),valid_x2.toarray(),valid_x3.toarray()],[valid_y,valid_y1]))\n",
    "    pred = model.predict([valid_x.toarray(),valid_x1.toarray(),valid_x2.toarray(),valid_x3.toarray()])\n",
    "    pred = [np.argmax(i) for i in pred[0]]\n",
    "    print(confusion_matrix(pred,valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#print(model.evaluate([valid_x.toarray(),valid_x3.toarray()],[valid_y,valid_y1]))\n",
    "#pred = model.predict([valid_x.toarray(),valid_x3.toarray()])\n",
    "print(model.evaluate([valid_x.toarray(),valid_x1.toarray(),valid_x2.toarray(),valid_x3.toarray()],[valid_y,valid_y1]))\n",
    "pred = model.predict([valid_x.toarray(),valid_x1.toarray(),valid_x2.toarray(),valid_x3.toarray()])\n",
    "pred = [np.argmax(i) for i in pred[0]]\n",
    "#pred = [1 if i>0.5 else 0 for i in pred]\n",
    "print(confusion_matrix(pred,valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存\n",
    "pred = [np.argmax(i) for i in pred[0]]\n",
    "with open('./submission9.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerow([\"user_id\", \"age_range\"])\n",
    "\n",
    "    for i, row in df_test.iterrows():\n",
    "        writer.writerow([row.user_id, pred[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 続いてsingle task model\n",
    "\n",
    "# single task\n",
    "def m(units=512):\n",
    "    i = Input(shape=(5000,))\n",
    "    x = Dense(units,activation='relu')(i)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(units//4,activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #\n",
    "    i1 = Input(shape=(20,))\n",
    "    i2 = Input(shape=(20,))\n",
    "    i3 = Input(shape=(20,))\n",
    "    #\n",
    "    j1 = Dense(16,activation='relu')(i1)\n",
    "    j2 = Dense(16,activation='relu')(i2)\n",
    "    j3 = Dense(16,activation='relu')(i3)\n",
    "    x = concatenate([x,j1])\n",
    "    #x = concatenate([x,j1,j2,j3])\n",
    "    x1 = Dense(units//4,activation='relu')(x)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    x1 = Dense(6,activation='softmax',name='age')(x1)\n",
    "    model = keras.Model(\n",
    "                                    #i,\n",
    "                                    [i,i1],\n",
    "                                    #[i,i1,i2,i3],\n",
    "                                    x1)\n",
    "    model.compile(\n",
    "                loss = 'sparse_categorical_crossentropy',\n",
    "                optimizer=keras.optimizers.Adam(1e-03),\n",
    "                #optimizer=keras.optimizers.SGD(),\n",
    "                metrics=['accuracy'],\n",
    "                )\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "for i in [32]:\n",
    "    model = m(units=1024)\n",
    "    hist = model.fit(\n",
    "        #train_x.toarray(),\n",
    "        [train_x.toarray(),train_x1.toarray()],\n",
    "        #[train_x.toarray(),train_x2.toarray()],\n",
    "        #[train_x.toarray(),train_x3.toarray()],\n",
    "        #[train_x.toarray(),train_x1.toarray(),train_x2.toarray(),train_x3.toarray()],\n",
    "        train_y,\n",
    "                 epochs=100,batch_size=32,verbose=2,\n",
    "                 callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss',verbose=2,patience=5)],\n",
    "                validation_data = (\n",
    "                    #valid_x.toarray(), \n",
    "                    [valid_x.toarray(),valid_x1.toarray()],\n",
    "                    #[valid_x.toarray(),valid_x2.toarray()],\n",
    "                    #[valid_x.toarray(),valid_x3.toarray()],\n",
    "                    #[valid_x.toarray(),valid_x1.toarray(),valid_x2.toarray(),valid_x3.toarray()],\n",
    "                    valid_y))\n",
    "    print(model.evaluate([valid_x.toarray(),valid_x1.toarray()],valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#評価\n",
    "\n",
    "print(model.evaluate(valid_x.toarray(),valid_y))\n",
    "pred = model.predict(valid_x.toarray())\n",
    "pred = [np.argmax(i) for i in pred]\n",
    "print(confusion_matrix(pred,valid_y))\n",
    "\n",
    "# print(model.evaluate([valid_x.toarray(),valid_x1.toarray(),valid_x2.toarray(),valid_x3.toarray()],valid_y))\n",
    "# pred = model.predict([valid_x.toarray(),valid_x1.toarray(),valid_x2.toarray(),valid_x3.toarray()])\n",
    "# pred = [np.argmax(i) for i in pred]\n",
    "# print(confusion_matrix(pred,valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([test_x.toarray(),test_x3.toarray()])\n",
    "pred = [np.argmax(i) for i in pred]\n",
    "\n",
    "with open('./submission25.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    writer.writerow([\"user_id\", \"age_range\"])\n",
    "    for i, row in df_test.iterrows():\n",
    "        writer.writerow([row.user_id, pred[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
